
# ğŸšš Desafio Engenharia de Dados â€” LogÃ­stica

Este projeto foi desenvolvido como parte de um desafio tÃ©cnico para a vaga de Engenheiro de Dados Jr. Ele simula o processamento de dados logÃ­sticos de rastreamento de pacotes, utilizando boas prÃ¡ticas de arquitetura em camadas (Bronze, Silver, Gold), orquestraÃ§Ã£o com Airflow, persistÃªncia em PostgreSQL e visualizaÃ§Ã£o com Power BI.

## ğŸ”§ Tecnologias Utilizadas

- Python + Pandas
- Airflow
- PostgreSQL
- Power BI
- Matplotlib + Seaborn (visualizaÃ§Ã£o exploratÃ³ria)

## ğŸ“¦ Arquitetura em Camadas

- **Bronze** â†’ IngestÃ£o de dados brutos com rastreabilidade
- **Silver** â†’ Limpeza, enriquecimento e particionamento por data
- **Gold** â†’ AgregaÃ§Ãµes finais, persistÃªncia em banco e visualizaÃ§Ãµes

---

## ğŸ¥‰ Camada Bronze â€” IngestÃ£o de Dados Brutos

### ğŸ“Œ Objetivo

Armazenar os dados exatamente como foram recebidos, sem alteraÃ§Ãµes, garantindo rastreabilidade, organizaÃ§Ã£o e escalabilidade para as prÃ³ximas camadas do pipeline.

### ğŸ“ Estrutura de Pastas

```
Desafio_IZE/
â”œâ”€â”€ dados/
â”‚   â””â”€â”€ rastreamento.csv
â”‚   â””â”€â”€ bronze/
â”‚       â”œâ”€â”€ resultado_rastreamento_bronze.csv
â”‚       â””â”€â”€ metadados_YYYYMMDD_HHMMSS.json
```

### âš™ï¸ O que o script faz

- LÃª o arquivo bruto `rastreamento.csv`
- Padroniza os nomes das colunas
- Salva o CSV tratado na pasta `bronze/`
- Gera um arquivo de metadados com:
  - Caminho do arquivo
  - Camada
  - Timestamp
  - Total de linhas
  - Nome e tipo das colunas

### â–¶ï¸ Como executar

```bash
python scripts/resultado_rastreamento_bronze.py
```

---

## ğŸ¥ˆ Camada Silver â€” Processamento e Tratamento

### ğŸ“Œ Objetivo

Aplicar regras de negÃ³cio, limpar e enriquecer os dados para tornÃ¡-los confiÃ¡veis e prontos para anÃ¡lise. Os dados sÃ£o particionados por ano e mÃªs, simulando mÃºltiplas execuÃ§Ãµes e facilitando auditoria.

### ğŸ“ Estrutura de Pastas

```
Desafio_IZE/
â”œâ”€â”€ dados/
â”‚   â””â”€â”€ silver/
â”‚       â””â”€â”€ ano=2025/
â”‚           â””â”€â”€ mes=10/
â”‚               â”œâ”€â”€ resultado_rastreamento_silver.csv
â”‚               â””â”€â”€ metadados_YYYYMMDD_HHMMSS.json
```

### âš™ï¸ O que o script faz

- LÃª o arquivo da camada Bronze
- Remove duplicatas e valores nulos
- Converte `data_atualizacao` para datetime
- Cria colunas derivadas: `ano`, `mes`, `dia`, `hora`
- Valida os valores de `status_rastreamento`
- Salva o CSV tratado em partiÃ§Ãµes por data
- Gera metadados atualizados, incluindo duplicatas

### â–¶ï¸ Como executar

```bash
python scripts/resultado_rastreamento_silver.py
```

---

## ğŸ¥‡ Camada Gold â€” Armazenamento e VisualizaÃ§Ã£o

### ğŸ“Œ Objetivo

Persistir os dados tratados em banco de dados relacional e gerar visualizaÃ§Ãµes analÃ­ticas.

### ğŸ“ Estrutura de Pastas

```
Desafio_IZE/
â”œâ”€â”€ dados/
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ resultado_rastreamento_gold.csv
â”‚       â””â”€â”€ metadados_YYYYMMDD_HHMMSS.json
```

### âš™ï¸ O que o script faz

- LÃª o arquivo mais recente da camada Silver
- Realiza agregaÃ§Ãµes e anÃ¡lises (ex: total por status, volume por cidade)
- Exporta os dados finais para PostgreSQL
- Salva o CSV final na pasta `gold/`
- Gera metadados finais

### â–¶ï¸ Como executar

```bash
python scripts/resultado_rastreamento_gold.py
```

---

## ğŸ“Š VisualizaÃ§Ãµes com Python â€” Explorando a Camada Gold

AlÃ©m da integraÃ§Ã£o com Power BI, foram desenvolvidas visualizaÃ§Ãµes exploratÃ³rias com Python utilizando as bibliotecas `matplotlib` e `seaborn`. Essa etapa nÃ£o era obrigatÃ³ria no desafio, mas foi implementada como diferencial tÃ©cnico.

### GrÃ¡ficos gerados:

- **GrÃ¡fico de Barras â€” NÃºmero de Pacotes por Status Final**  
  Representa a distribuiÃ§Ã£o dos pacotes entre os status `sucesso`, `falha` e `pendente`.

- **GrÃ¡fico de Linha â€” Volume de Envios por Dia**  
  Exibe a quantidade de pacotes movimentados por dia, com correÃ§Ã£o de datas para evitar sobreposiÃ§Ã£o entre meses e anos.

- **GrÃ¡fico de Linha â€” Volume de Envios por Data Completa**  
  Utiliza a composiÃ§Ã£o de `ano`, `mes` e `dia` para gerar uma linha do tempo precisa, permitindo identificar tendÃªncias e picos de movimentaÃ§Ã£o.

### â–¶ï¸ Como executar

```bash
python scripts/visualizacao_gold.py
```

Os grÃ¡ficos serÃ£o salvos automaticamente na pasta `dados/gold/` como arquivos `.png`.

---

## ğŸ§± Modelo Relacional â€” Desafio 1, QuestÃ£o 2

```sql
CREATE TABLE Pacotes (
  id_pacote VARCHAR PRIMARY KEY,
  origem TEXT,
  destino TEXT
);

CREATE TABLE EventosRastreamento (
  id_evento SERIAL PRIMARY KEY,
  id_pacote VARCHAR REFERENCES Pacotes(id_pacote),
  status_rastreamento TEXT,
  data_atualizacao TIMESTAMP
);
```

### ğŸ” Justificativa

- Permite mÃºltiplos eventos por pacote, refletindo o histÃ³rico completo
- Facilita consultas analÃ­ticas (tempo mÃ©dio, status atual)
- Garante integridade referencial
- EscalÃ¡vel para grandes volumes e integraÃ§Ã£o futura com APIs

---

## âš™ï¸ Justificativa das Tecnologias â€” Desafio 1, QuestÃ£o 3

- **Python + Pandas**: manipulaÃ§Ã£o rÃ¡pida e eficiente de dados tabulares
- **PostgreSQL**: banco relacional robusto e compatÃ­vel com BI
- **Power BI**: visualizaÃ§Ã£o interativa e acessÃ­vel
- **Airflow**: orquestraÃ§Ã£o confiÃ¡vel e escalÃ¡vel para pipelines em produÃ§Ã£o

> Ferramentas como Spark ou Dataflow foram consideradas desnecessÃ¡rias para o volume atual.

---

## ğŸ“¦ Desafio 2 â€” Monitoramento e EficiÃªncia LogÃ­stica

### ğŸ§© QuestÃ£o 1 â€” Dashboard em Tempo Quase Real

Foi proposta uma soluÃ§Ã£o com:

- IngestÃ£o via Airflow agendado
- Processamento incremental com scripts Python
- Armazenamento em PostgreSQL
- VisualizaÃ§Ã£o com Power BI

#### ğŸ“ˆ MÃ©tricas simuladas:

- NÃºmero de pacotes por status
- MÃ©dia de tempo de entrega (via SQL)
- Volume por origem/destino

---

### ğŸ”Œ QuestÃ£o 2 â€” AdaptaÃ§Ã£o para IngestÃ£o via API

Embora nÃ£o implementado, foi documentado um esboÃ§o de arquitetura futura:

#### ğŸ§  MudanÃ§as necessÃ¡rias:

- Substituir leitura de CSV por API (FastAPI, Kafka)
- Processamento em tempo real
- PersistÃªncia incremental
- ValidaÃ§Ã£o e ordenaÃ§Ã£o dos dados

#### ğŸ—‚ï¸ Modelo adaptado:

Cada evento vira uma linha com timestamp, mantendo histÃ³rico completo.

#### ğŸ§ª Arquitetura proposta:

```
[API de rastreamento] â†’ [Kafka/FastAPI] â†’ [Spark Streaming] â†’ [PostgreSQL] â†’ [Power BI]
```

---

## ğŸ“£ ObservaÃ§Ãµes Finais

Este pipeline foi desenvolvido com foco em:

- ğŸ§¼ Boas prÃ¡ticas de engenharia de dados
- ğŸ” Escalabilidade e modularidade
- ğŸ” Rastreabilidade via metadados
- ğŸ“Š VisualizaÃ§Ã£o clara e objetiva dos dados logÃ­sticos
